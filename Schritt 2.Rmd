---
title: "Schritt 2"
output: html_document
---
```{r}
library(ggcorrplot)
library(dplyr)
library(ggplot2)
library(gridExtra)
```

```{r}
df_1 <-read.csv("cardio_train.csv", sep=";")
df_1$age <- df_1$age/365
df_1$height <- df_1$height/100
df_1 <- cbind(df_1 , "BMI" = (df_1$weight)/(df_1$height)^2 )
df_1$height<- NULL
df_1$weight <- NULL
head(df_1)
```

Schritt 2 - Untersuchen der Gruppierungen



Gruppe 1 (Gute Gesundheit) 
- BMI 18 bis 25
ap_lo <80
ap_hi <130


Gruppe 2 (Übergewicht/ Untergewicht und Bluthochdruck):
- BMI >25
- BMI <18
- ap_lo >80
- ap_hi >130

```{r}
df_1$id <- NULL
df_gut<-df_1 %>% filter(BMI >= 18) %>% filter(BMI <= 25) %>% filter(ap_lo<=89) %>%filter(ap_hi<=139)
df_schlecht_1<-df_1 %>% filter(BMI < 18, ap_lo > 89, ap_hi > 139)
df_schlecht_2<-df_1 %>% filter(BMI > 25, ap_lo > 89, ap_hi > 139)
df_schlecht <- rbind(df_schlecht_1,df_schlecht_2)
nrow(df_schlecht)
nrow(df_gut)
head(df_gut)

```

!! fast 4000 weniger Datenpunkte. Evtl macht es hier also sinn die Blutdruck werte aussenvor zu lassen?
TB Discussed

- Cholesterol und Glucose nicht weiter da das "Essensangewohnheiten"



Schritt 2: Untersuchen der einzelnen Gruppen und aufzeigen verschiedener Korrelationen innerhalb der Gruppe

Kategorie 1: Ernährung
- Betrachtete Variablen:
1. Cholesterol
2. Glucose

```{r}
#Glucose gesund
df_gluc1 <- data.frame(c(1,2,3),c(nrow(filter(df_gut,gluc == 1))/nrow(df_gut),nrow(filter(df_gut,gluc == 2))/nrow(df_gut),nrow(filter(df_gut,gluc == 3))/nrow(df_gut)))
colnames(df_gluc1) <- c("Glucose","Ratio")

#Cholesterol gesund
df_chol1 <- data.frame(c(1,2,3),c(nrow(filter(df_gut,cholesterol == 1))/nrow(df_gut),nrow(filter(df_gut,cholesterol == 2))/nrow(df_gut),nrow(filter(df_gut,cholesterol == 3))/nrow(df_gut)))
colnames(df_chol1) <- c("Cholesterol","Ratio")

#Glucose ungesund
df_gluc2 <- data.frame(c(1,2,3),c(nrow(filter(df_schlecht,gluc == 1))/nrow(df_schlecht),nrow(filter(df_schlecht,gluc == 2))/nrow(df_schlecht),nrow(filter(df_schlecht,gluc == 3))/nrow(df_schlecht)))
colnames(df_gluc2) <- c("Glucose","Ratio")

#Cholesterol ungesund
df_chol2 <- data.frame(c(1,2,3),c(nrow(filter(df_schlecht,cholesterol == 1))/nrow(df_schlecht),nrow(filter(df_schlecht,cholesterol == 2))/nrow(df_schlecht),nrow(filter(df_schlecht,cholesterol == 3))/nrow(df_schlecht)))
colnames(df_chol2) <- c("Cholesterol","Ratio")




plt1<-ggplot(df_gluc1, aes(x=Glucose, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Glucose Level bei guter Gesundheit")
plt2<-ggplot(df_chol1, aes(x=Cholesterol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Cholesterol Level bei guter Gesundheit")
plt3<-ggplot(df_gluc2, aes(x=Glucose, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Glucose Level bei schlechter Gesundheit")
plt4<-ggplot(df_chol2, aes(x=Cholesterol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Cholesterol Level bei schlechter Gesundheit")
grid.arrange(plt1,plt2,plt3,plt4, ncol=2)
```
Kategorie 2: Drogenkonsum
- Betrachtete Variablen:
1. alco
2. smoke

```{r}
#Alkohol gesund
df_alc1 <- data.frame(c(0,1),c(nrow(filter(df_gut,alco == 0))/nrow(df_gut),nrow(filter(df_gut,alco == 1))/nrow(df_gut)))
colnames(df_alc1) <- c("Alkohol","Ratio")

#Rauchen gesund
df_smoke1 <- data.frame(c(0,1),c(nrow(filter(df_gut,smoke == 0))/nrow(df_gut),nrow(filter(df_gut,smoke == 1))/nrow(df_gut)))
colnames(df_smoke1) <- c("Rauchen","Ratio")

#Alkohol ungesund
df_alc2 <- data.frame(c(0,1),c(nrow(filter(df_schlecht,alco == 0))/nrow(df_schlecht),nrow(filter(df_schlecht,alco == 1))/nrow(df_schlecht)))
colnames(df_alc2) <- c("Alkohol","Ratio")

#Rauchen ungesund
df_smoke2 <- data.frame(c(0,1),c(nrow(filter(df_schlecht,smoke == 0))/nrow(df_schlecht),nrow(filter(df_schlecht,smoke == 1))/nrow(df_schlecht)))
colnames(df_smoke2) <- c("Rauchen","Ratio")




plt5<-ggplot(df_alc1, aes(x=Alkohol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Alkoholkonsumenten bei guter Gesundheit")
plt6<-ggplot(df_smoke1, aes(x=Rauchen, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Raucher bei guter Gesundheit")
plt7<-ggplot(df_alc2, aes(x=Alkohol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Alkoholkonsumenten bei schlechter Gesundheit")
plt8<-ggplot(df_smoke2, aes(x=Rauchen, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Raucher bei schlechter Gesundheit")
grid.arrange(plt5,plt6,plt7,plt8, ncol=2)
```
Kategorie 3: Sportliche Betätigung
- Betrachtete Variablen:
1. active

```{r}
#Sport gesund
df_act1 <- data.frame(c(0,1),c(nrow(filter(df_gut,active == 0))/nrow(df_gut),nrow(filter(df_gut,active == 1))/nrow(df_gut)))
colnames(df_act1) <- c("Sport","Ratio")

#Sport ungesund
df_act2 <- data.frame(c(0,1),c(nrow(filter(df_schlecht,active == 0))/nrow(df_schlecht),nrow(filter(df_schlecht,active == 1))/nrow(df_schlecht)))
colnames(df_act2) <- c("Sport","Ratio")


plt9<-ggplot(df_act1, aes(x=Sport, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Körperliche Betätigung bei guter Gesundheit")

plt10<-ggplot(df_act2, aes(x=Sport, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Körperliche Betätigung bei schlechter Gesundheit")

grid.arrange(plt9,plt10, ncol=2)
```
Next Steps:
- Lineare Regressionsmodell zur Modelierung der Wahrscheinlichkeit einer Herzkrankheit


Welche Lineare Regression ist hier sinnvoll?
Stichwort: Lineare Regression mit einer bivariaten abhängigen Variable
- wir nehmen das im Interent vorgeschlagene linear probabilty model (https://www.econometrics-with-r.org/11-rwabdv.html)



Beschreibung des linear probability models:
- Wir haben zwei vershcieden Werte die Y_i annehmen kann. Einmal Y_i = 1 und Y_i = 0 in abhänigkeit unserer Einflussgrößen X_i_n. Y_i ist also Bernoulli verteilt mit Erwartungswert p. P ist entsprechend die Wahrscheinlichkeit für Y=1 in Abhängigkeit gegebener Einflussgrößern.
- Die beta werden also interpretiert als veränderung der Wahrscheinlichkeit für Y_i=1 bei fixierung aller anderen unabhängigen Variablen
- Die Berechnung der beta kann hierbei mit dem OLS verfahren berechnet werden 
- Ein großes Problem beim verwenden einer lineare Regression ist, dass sie den zusammenhang zwischen Wahrscheinlichkeit dass Y_i=1 und den unabhänigen Variablen als linear betrachtet. Entsprechend können prognosen 1 übersteigen

Benötige hierzu:
- alpha
- alle betas zu den einzelnen Variablen: kontinuierliche: BMI, Blutdruck, Cholisterol
                                        disket: Glucose, Alkohol, Rauchen, Sport
                                        
                                        
#Teil 3 Methoden: Probit Regression 

Aufgrund der vorliegenden Erkenntnisse aus der Explorativen Datenanalyse, erscheint es sinnvoll die versteckte Beziehung zwischen den Variablen genauer zu untersuchen und über eine Regression in linearen Zusammenhang zu der Variable $Cardio$ zu stellen. \newline
Die binäre Variable $Cardio$ liefert nicht die gewünschte Feinheit, durch welche Änderungen in der abhängigen Größe auf die unabhängigen Einflussvariablen zurückgespielt werden können.
Ohne Einschränkung der Aussagekraft wird daher statt der $Cardio$ Variable, ihr, von den Einflussvariablen, abhängiger Erwartungswert beschrieben. \newline
Jedoch besteht für die klassische lineare Regression weiterhin große Fehleranfälligkeit. Zum einen führt der beschriebene Lineare Zusammenhang zu erwarteten Wahrscheinlichkeiten die über $1$ liegen und zum anderen sind aus der Vorlesung bekannte Tests zur Genauigkeit der Regression wenig Aussagekräftig.\newline
Um das erstere Problem zu umgehen findet die Probit Regression Anwendung.
Hierbei wird der bedingte Erwartungswert der abhängigen Variable über die Verteilungsfunktion der Normalverteilung berechnet. Die Regression modeliert dabei den $z$-Wert, welcher schließlich über den $qnorm$ Befehl zu der erhaltenen Vorhersage $\Phi(z)$ führt. Die hierfür verwendete Formel lautet\footnote{https://www.econometrics-with-r.org/11.2-palr.html}: \par

\begin{equation*}
P(Y=1|X_1,X_2,...,X_k)= \Phi(\beta_0+\beta_1*X_1+...+\beta_k*X_k)
\end{equation*}

für das Modell

\begin{equation*}
Y= \beta_0+\beta_1*X_1+...+\beta_k*X_k+u
\end{equation*}

Dabei wird angenommen, dass die bedingten Erwartungswerte Normalverteilt sind. Die Unabhängigkeit der Variablen ist durch die Zufällige Probe der Personen gegeben.
Die Deutung der berechneten Koeffizienten wird intuitiv wenn man die Veränderung des z-Wertes bei isolierter Veränderung der jeweiligen unabhängigen Variable betrachtet. 

```{r, echo=FALSE}
glm.model <- glm(cardio ~ age+gender+BMI+ap_hi+ap_lo+cholesterol+gluc+smoke+alco+active,family = binomial(link = "probit"),data=df_1, control = list(maxit = 100))

summary(glm.model)
```

Die folgende Zusammenfassung der Probit Regression wird mit der Funktion $summary$ erzeugt. Besonders wichtig sind hierzu die werte der jeweiligen Koeffizienten sowie der p-Werte der Hypothesentest entgegen der Nullhypothese “der Koeffizient ist gleich 0”. Dies lässt auf die signifikanz des Koeffizienten schließen.\newline
Die Visualisierung der Prognosen bezüglich der abhängigen Variabel $BMI$ verdeutlichen die unterschiede welche eine Probit Regression im Vergleich zu einer Linearen Regression haben. 

# Plot der Predictions mit Realen daten (Visualiesierung der Prognosed daten)
```{r , echo=FALSE}
ggplot(df_predict, aes(x=BMI , y=cardio)) + xlim(0, 100) + geom_point() + geom_smooth(method = 'glm', method.args = list(family = binomial(link = "probit"))) +ggtitle("Probit Model gegeben der Veränderung des BMI") +ylab("Risiko einer kardiovaskulären Erkrankung")

```
                                    


#Methoden: Korrelationspyramide

Die folgende Korrelationspyramide zeigt die jeweiligen Korrelationen der untersuchten Variablen auf. Dabei wird besonderes Augenmerk auf die Korrelation der unabhängigen Variablen mit der abhängigen Variablen gesetzt, da diese die Aussagekraft des Models untermalen. Die berechnung und visualisierung übernimmt hierbei das ggcorrplot package \footnote{“https://cran.r-project.org/web/packages/corrplot/index.html”} welches zur Berechnung der Relevanten Werte auf die Funktion $cor.test$ aus dem Packet $stats$ zurückgreift. Die aufgezeigten Werte gleichen entsprechend dem empirischen Korrelationskoeffizienten \footnote{Definition 6.38 aus dem Vorlesungsskript}.
Mittels eines p.Test werden die Werte in einem weiteren Plot gegen die Nullhypothese $H.0 := Der \,Korrelationskoeffizient \, ist \,gleich \,0$ getestet. Die Werte die hierbei nicht verworfen werden können, sind dabei mit einem $X$ gekennzeichnet.

```{r, echo=FALSE}

corr <- cor(df_1)
corr
p.mat <- cor_pmat(df_1)

ggcorrplot(corr, hc.order = TRUE, type="lower", p.mat=p.mat)
```
```{r, echo=FALSE}
library(tidyverse)
ggcorrplot(corr, hc.order = TRUE, type="lower", lab=TRUE)


```



#Teil 4 Auswertung der Ergebnisse: Auswertung der Regression


```{r, echo=FALSE}
#datasets for prediction tutti
df_predict <- df_1
prediction_tot <- predict(glm.model, newdata = df_predict, type="response")
df_predict$prediction <- prediction_tot 
df_predict

#datasets for prediction gut
df_gut_predict <- df_gut
df_gut_predict$cardio <- NULL
df_gut_predict$id <- NULL

prediction_gut <- predict(glm.model, newdata = df_gut_predict,type="response")
df_gut_predict$prediction <- prediction_gut 


#datasets for prediction schlecht
df_schlecht_predict <- df_schlecht
df_schlecht_predict$cardio <- NULL
df_schlecht_predict$id <- NULL

prediction_schlecht <- predict(glm.model, newdata = df_schlecht_predict,type="response")
df_schlecht_predict$prediction <- prediction_schlecht

```

Die $p$-Werte liefern, wie bereits erwähnt, eine Aussage bezüglich der Signifikanz der berechneten $\beta$-Koeffizienten. Intuitiv folgt daraus, dass eine Veränderung in der unabhängigen Variable über die betrachtete unabhängige Variable erklärt werden kann. \newline
Aus der summary des Models wird sofort ersichtlich, dass jede der gelisteten Variable signifikant zu einem Nivea sind welches kleiner als $\alpha = 0,1\%$ ist. Es lässt sich also schließen, dass zu jeder abhängigen Variable, die Bewegung zur Veränderung des $z$-Wertes führt. \newline
Ähnliches lässt sich auch von der Korrelationspyramide ableiten. Hier weist keine Variable eine insignifikante korrelation mit der unabhängigen Variable auf. Auffallend ist dabei, dass die Variablen: $BMI$ und $Cholesterol$ am stärksten mit $Cardio$ korrelieren. Interessant ist auch, dass das Geschlecht ebenfalls einen Einfluss auf das Risiko hat an kardiovaskulären Krankheiten zu erleiden. Dies deckt sich mit der Datenanalyse aus Teil 2 (überprüfen). \par

```{r , echo=FALSE}
summary(glm.model)$coefficients
```


Um Aussagen über die Genauigkeit des Tests treffen zu können, werden die Erkentnisse der explorativen Datenanalyse genommen und mit den Charakteristiken der geschätzten Werte verglichen. 
Die Histogramme wurden entsprechen der geschilderten Gruppenaufteilung des Datensets angefertigt und darüber eine ideale Normalverteilung (in blau) gelegt, mit jeweiliger verschiedener Varianzen und Erwartungswerten. Es lässt sich deutlich erkennen, dass die Personen mit schlechter Gesundheit ein höheres Risiko und die mit guter Gesundheit ein niedrigere Risiko aufweisen, zu erkranken. Auch interessant ist, dass eine höhere Menge an Personen in unserem Gesamt Datenset ein niedrigeres Risiko aufweisen. Der Erwartungswert liegt bei  $\sim50\%$.

```{r , echo=FALSE}
x<-df_predict$prediction
z <- df_gut_predict$prediction
b<-df_schlecht_predict$prediction
h.x<-hist(x, main="Verteilung der Gesamtgruppe", xlab="Risiko zu erkranken", ylab="Anzahl der Personen", ylim=c(0,8000))
xfit<-seq(min(x),max(x),length=40)

y.xfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
y.xfit <- y.xfit*diff(h.x$mids[1:2])*length(x)
lines(xfit, y.xfit, col="blue", lwd=2)
h.b<-hist(b, main="Verteilung der Gruppe mit schlechter Gesundheit", xlab="Risiko zu erkranken", ylab="Anzahl der Personen",ylim=c(0,2000))
bfit <- seq(min(b),max(b),length=40)
y.bfit<-dnorm(bfit,mean=mean(b),sd=sd(b))
y.bfit <- y.bfit*diff(h.b$mids[1:2])*length(b)
lines(bfit, y.bfit, col="blue", lwd=2)
h.z<-hist(z, main="Verteilung der Gruppe mit guter Gesundheit", xlab="Risiko zu erkranken", ylab="Anzahl der Personen")
zfit <- seq(min(z),max(z),length=40)
y.zfit<-dnorm(zfit,mean=mean(z),sd=sd(z))
y.zfit <- y.zfit*diff(h.z$mids[1:2])*length(z)
lines(zfit, y.zfit, col="blue", lwd=2)


```

```{r , echo=FALSE}
sum(df_1$cardio)/length(df_1$cardio)
```


```{r , echo=FALSE}
# Compare prediction and real values

mean_tut <- mean(df_1$cardio)
mean_gut <- mean(df_gut$cardio)
mean_schlecht <- mean(df_schlecht$cardio)
mean_pred_tut <- mean(df_predict$prediction)
mean_pred_gut <- mean(df_gut_predict$prediction)
mean_pred_schlecht <- mean(df_schlecht_predict$prediction)

```

Die folgende Tabelle veranschaulicht die Abweichung der Prognose von den reellen Daten. Aus der Vorlesung ist bekannt, dass der empirische Mittelwert annähernd Normalverteilt ist, entsprechend läuft der Vergleich nach dem Prinzip des mittleren quadratischen Fehlers mit $n=1$.  \newline
Es wird ersichtlich, dass das Model die Daten gut erklärt. Lediglich bei der Anwendung des Models auf die Gruppe mit Guter Gesundheit prognostiziert das Model ein grundsätzlich höheres Risiko. 


```{r , echo=FALSE}
Abweichung <- function(x,y){
  z=(abs(y-x)/x)*100
  return(z)
}

compare_df <- data.frame(c("Total","Gut","Schlecht"), c(mean_tut, mean_gut, mean_schlecht),c(mean_pred_tut, mean_pred_gut, mean_pred_schlecht), c(Abweichung(mean_tut,mean_pred_tut),Abweichung(mean_gut,mean_pred_gut),Abweichung(mean_schlecht,mean_pred_schlecht)))
colnames(compare_df) <- c("Betrachtete Gruppe", "Wahrer Median", "Model Median", "Prozentuale Abweichung")

compare_df
```
```{r , echo=FALSE}

```






