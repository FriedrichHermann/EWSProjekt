---
title: "Schritt 2"
output: html_document
---
```{r}
library(ggcorrplot)
library(dplyr)
library(ggplot2)
library(gridExtra)
```

```{r}
df_1 <-read.csv("cardio_train.csv", sep=";")
df_1$age <- df_1$age/365
df_1$height <- df_1$height/100
df_1 <- cbind(df_1 , "BMI" = (df_1$weight)/(df_1$height)^2 )
head(df_1)
```

Schritt 2 - Untersuchen der Gruppierungen



Gruppe 1 (Gute Gesundheit) 
- BMI 18 bis 25
ap_lo <80
ap_hi <130


Gruppe 2 (Übergewicht/ Untergewicht und Bluthochdruck):
- BMI >25
- BMI <18
- ap_lo >80
- ap_hi >130

```{r}
df_gut<-df_1 %>% filter(BMI >= 18) %>% filter(BMI <= 25) %>% filter(ap_lo<=89) %>%filter(ap_hi<=139)
df_schlecht_1<-df_1 %>% filter(BMI < 18, ap_lo > 89, ap_hi > 139)
df_schlecht_2<-df_1 %>% filter(BMI > 25, ap_lo > 89, ap_hi > 139)
df_schlecht <- rbind(df_schlecht_1,df_schlecht_2)
nrow(df_schlecht)
nrow(df_gut)
head(df_gut)

```

!! fast 4000 weniger Datenpunkte. Evtl macht es hier also sinn die Blutdruck werte aussenvor zu lassen?
TB Discussed

- Cholesterol und Glucose nicht weiter da das "Essensangewohnheiten"



Schritt 2: Untersuchen der einzelnen Gruppen und aufzeigen verschiedener Korrelationen innerhalb der Gruppe

Kategorie 1: Ernährung
- Betrachtete Variablen:
1. Cholesterol
2. Glucose

```{r}
#Glucose gesund
df_gluc1 <- data.frame(c(1,2,3),c(nrow(filter(df_gut,gluc == 1))/nrow(df_gut),nrow(filter(df_gut,gluc == 2))/nrow(df_gut),nrow(filter(df_gut,gluc == 3))/nrow(df_gut)))
colnames(df_gluc1) <- c("Glucose","Ratio")

#Cholesterol gesund
df_chol1 <- data.frame(c(1,2,3),c(nrow(filter(df_gut,cholesterol == 1))/nrow(df_gut),nrow(filter(df_gut,cholesterol == 2))/nrow(df_gut),nrow(filter(df_gut,cholesterol == 3))/nrow(df_gut)))
colnames(df_chol1) <- c("Cholesterol","Ratio")

#Glucose ungesund
df_gluc2 <- data.frame(c(1,2,3),c(nrow(filter(df_schlecht,gluc == 1))/nrow(df_schlecht),nrow(filter(df_schlecht,gluc == 2))/nrow(df_schlecht),nrow(filter(df_schlecht,gluc == 3))/nrow(df_schlecht)))
colnames(df_gluc2) <- c("Glucose","Ratio")

#Cholesterol ungesund
df_chol2 <- data.frame(c(1,2,3),c(nrow(filter(df_schlecht,cholesterol == 1))/nrow(df_schlecht),nrow(filter(df_schlecht,cholesterol == 2))/nrow(df_schlecht),nrow(filter(df_schlecht,cholesterol == 3))/nrow(df_schlecht)))
colnames(df_chol2) <- c("Cholesterol","Ratio")




plt1<-ggplot(df_gluc1, aes(x=Glucose, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Glucose Level bei guter Gesundheit")
plt2<-ggplot(df_chol1, aes(x=Cholesterol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Cholesterol Level bei guter Gesundheit")
plt3<-ggplot(df_gluc2, aes(x=Glucose, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Glucose Level bei schlechter Gesundheit")
plt4<-ggplot(df_chol2, aes(x=Cholesterol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Cholesterol Level bei schlechter Gesundheit")
grid.arrange(plt1,plt2,plt3,plt4, ncol=2)
```
Kategorie 2: Drogenkonsum
- Betrachtete Variablen:
1. alco
2. smoke

```{r}
#Alkohol gesund
df_alc1 <- data.frame(c(0,1),c(nrow(filter(df_gut,alco == 0))/nrow(df_gut),nrow(filter(df_gut,alco == 1))/nrow(df_gut)))
colnames(df_alc1) <- c("Alkohol","Ratio")

#Rauchen gesund
df_smoke1 <- data.frame(c(0,1),c(nrow(filter(df_gut,smoke == 0))/nrow(df_gut),nrow(filter(df_gut,smoke == 1))/nrow(df_gut)))
colnames(df_smoke1) <- c("Rauchen","Ratio")

#Alkohol ungesund
df_alc2 <- data.frame(c(0,1),c(nrow(filter(df_schlecht,alco == 0))/nrow(df_schlecht),nrow(filter(df_schlecht,alco == 1))/nrow(df_schlecht)))
colnames(df_alc2) <- c("Alkohol","Ratio")

#Rauchen ungesund
df_smoke2 <- data.frame(c(0,1),c(nrow(filter(df_schlecht,smoke == 0))/nrow(df_schlecht),nrow(filter(df_schlecht,smoke == 1))/nrow(df_schlecht)))
colnames(df_smoke2) <- c("Rauchen","Ratio")




plt5<-ggplot(df_alc1, aes(x=Alkohol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Alkoholkonsumenten bei guter Gesundheit")
plt6<-ggplot(df_smoke1, aes(x=Rauchen, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Raucher bei guter Gesundheit")
plt7<-ggplot(df_alc2, aes(x=Alkohol, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Alkoholkonsumenten bei schlechter Gesundheit")
plt8<-ggplot(df_smoke2, aes(x=Rauchen, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Raucher bei schlechter Gesundheit")
grid.arrange(plt5,plt6,plt7,plt8, ncol=2)
```
Kategorie 3: Sportliche Betätigung
- Betrachtete Variablen:
1. active

```{r}
#Sport gesund
df_act1 <- data.frame(c(0,1),c(nrow(filter(df_gut,active == 0))/nrow(df_gut),nrow(filter(df_gut,active == 1))/nrow(df_gut)))
colnames(df_act1) <- c("Sport","Ratio")

#Sport ungesund
df_act2 <- data.frame(c(0,1),c(nrow(filter(df_schlecht,active == 0))/nrow(df_schlecht),nrow(filter(df_schlecht,active == 1))/nrow(df_schlecht)))
colnames(df_act2) <- c("Sport","Ratio")


plt9<-ggplot(df_act1, aes(x=Sport, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Körperliche Betätigung bei guter Gesundheit")

plt10<-ggplot(df_act2, aes(x=Sport, y=Ratio))  + geom_bar(stat = "identity") + ggtitle("Körperliche Betätigung bei schlechter Gesundheit")

grid.arrange(plt9,plt10, ncol=2)
```
Next Steps:
- Lineare Regressionsmodell zur Modelierung der Wahrscheinlichkeit einer Herzkrankheit


Welche Lineare Regression ist hier sinnvoll?
Stichwort: Lineare Regression mit einer bivariaten abhängigen Variable
- wir nehmen das im Interent vorgeschlagene linear probabilty model (https://www.econometrics-with-r.org/11-rwabdv.html)



Beschreibung des linear probability models:
- Wir haben zwei vershcieden Werte die Y_i annehmen kann. Einmal Y_i = 1 und Y_i = 0 in abhänigkeit unserer Einflussgrößen X_i_n. Y_i ist also Bernoulli verteilt mit Erwartungswert p. P ist entsprechend die Wahrscheinlichkeit für Y=1 in Abhängigkeit gegebener Einflussgrößern.
- Die beta werden also interpretiert als veränderung der Wahrscheinlichkeit für Y_i=1 bei fixierung aller anderen unabhängigen Variablen
- Die Berechnung der beta kann hierbei mit dem OLS verfahren berechnet werden 
- Ein großes Problem beim verwenden einer lineare Regression ist, dass sie den zusammenhang zwischen Wahrscheinlichkeit dass Y_i=1 und den unabhänigen Variablen als linear betrachtet. Entsprechend können prognosen 1 übersteigen

Benötige hierzu:
- alpha
- alle betas zu den einzelnen Variablen: kontinuierliche: BMI, Blutdruck, Cholisterol
                                        disket: Glucose, Alkohol, Rauchen, Sport
```{r}
library(ggplot2)
df_2 <- df_1
df_2$id <- NULL
df_2$BMI <- NULL
colnames(df_2)
model <- lm(cardio ~ age+gender+height+weight+ap_hi+ap_lo+cholesterol+gluc+smoke+alco+active,data=df_2  )
summary(model)

chisq.test(model$fitted.values, df_2$cardio)
sigma(model)/mean(df_2$cardio)
```



p value in regression
- testet die Nullhypothes für jedes Argument, dass dieses gleich 0 ist
- Der p-Wert beschreibt hierbei, bei welchem signifikanzlevel dieses verworfen werden konnte. Für signifikante variablen sucht man meist nach p-werten die kleiner also 5% sind


Regressions koeffizienten
- beschreiben die veränderung des Wertes bei einer einheitsveränderung der beschriebenen variable wenn alle anderen variablen konstant gehalten werden


?Frage für die Runde. Kann ich den bekommenen wert als Wahrscheinlichkeit interpretieren dass die Menschen eine Herzstöung bekommen

Next Steps:
Aufzeigen der Korrelation zwischen variablen und cardio, sowie den variablen unter sich:

```{r}

corr <- cor(df_2)
corr
p.mat <- cor_pmat(df_2)
head(p.mat)

ggcorrplot(corr, hc.order = TRUE, type="lower", p.mat=p.mat)

```
```{r}
ggcorrplot(corr, hc.order = TRUE, type="lower", lab=TRUE)

```


Next Steps:
- Vergleiche die Median Wahrscheinlichkeit bei den Gruppen die wir als gesund und ungesund deklariert haben mit dem erhaltenen Regressionsmodell

```{r}
#datasets for prediction tutti
df_predict <- df_2
df_predict$cardio <- NULL
prediction_tot <- predict(model, newdata = df_predict)
df_predict$prediction <- prediction_tot 
df_predict

#datasets for prediction gut
df_gut_predict <- df_gut
df_gut_predict$cardio <- NULL
df_gut_predict$id <- NULL
df_gut_predict$BMI <- NULL
prediction_gut <- predict(model, newdata = df_gut_predict)
df_gut_predict$prediction <- prediction_gut 


#datasets for prediction schlecht
df_schlecht_predict <- df_schlecht
df_schlecht_predict$cardio <- NULL
df_schlecht_predict$id <- NULL
df_schlecht_predict$BMI <- NULL
prediction_schlecht <- predict(model, newdata = df_schlecht_predict)
df_schlecht_predict$prediction <- prediction_schlecht

```

Plot of "predicitons" vs Normal distribution

```{r}

x<-df_predict$prediction
z <- df_gut_predict$prediction
b<-df_schlecht_predict$prediction
h.x<-hist(x)
xfit<-seq(min(x),max(x),length=40)

y.xfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
y.xfit <- y.xfit*diff(h.x$mids[1:2])*length(x)
lines(xfit, y.xfit, col="blue", lwd=2)
h.b<-hist(b)
bfit <- seq(min(b),max(b),length=40)
y.bfit<-dnorm(bfit,mean=mean(b),sd=sd(b))
y.bfit <- y.bfit*diff(h.b$mids[1:2])*length(b)
lines(bfit, y.bfit, col="blue", lwd=2)
h.z<-hist(z)
zfit <- seq(min(z),max(z),length=40)
y.zfit<-dnorm(zfit,mean=mean(z),sd=sd(z))
y.zfit <- y.zfit*diff(h.z$mids[1:2])*length(z)
lines(zfit, y.zfit, col="blue", lwd=2)


```

```{r}
sum(df_1$cardio)/length(df_1$cardio)
```


```{r}
# Compare prediction and real values

mean_tut <- mean(df_1$cardio)
mean_gut <- mean(df_gut$cardio)
mean_schlecht <- mean(df_schlecht$cardio)
mean_pred_tut <- mean(df_predict$prediction)
mean_pred_gut <- mean(df_gut_predict$prediction)
mean_pred_schlecht <- mean(df_schlecht_predict$prediction)

```


```{r}
MSE <- function(x,y){
  z=0
  for (i in 1:length(x)){
    z=z+(x[i]-y[i])^2
  }
  return(z/length(x))
}
compare_df <- data.frame(c("Total","Gut","Schlecht"), c(mean_tut, mean_gut, mean_schlecht),c(mean_pred_tut, mean_pred_gut, mean_pred_schlecht), c(MSE(df_1$cardio, df_predict$prediction), MSE(df_gut$cardio, df_gut_predict$prediction), MSE(df_schlecht$cardio, df_schlecht_predict$prediction) ))
colnames(compare_df) <- c("Betrachtete Gruppe", "Wahrer Median", "Model Median", "Mittlere quadratische Fehler")

compare_df
```
```{r}

```

Textstruktur:
1. Genauere Beschreibung der linearen Regression sowie dem Koeffizientenbaum
2. Darüber hinaus auch beschriebung der p-Werte der Koeffizienten
3. Eingehen auf die verschiedenen tests die verwendet werden um die lineare Regression zu beschreiben
4. Vergleich der vorhergesagten Werte mit den wahren Werten





